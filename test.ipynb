{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13e41eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to task: 3cebe0e4059e4fa58c57bd9a650ef7f5\n",
      "Found artifact: 'Baseline Model'. Downloading and deserializing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception 'Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.' encountered when getting artifact with type pickle and content type application/pickle\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to task: ce89c3a50dda4ef1809314d2bce71374\n",
      "Found artifact: 'Test Dataloader'. Downloading and deserializing...\n",
      "Connecting to task: ce89c3a50dda4ef1809314d2bce71374\n",
      "Found artifact: 'Unlearning Dataset'. Downloading and deserializing...\n"
     ]
    }
   ],
   "source": [
    "from clearml import Task\n",
    "\n",
    "def load_model_from_task(task_id: str, artifact_name: str = \"trained Model\"):\n",
    "    \"\"\"\n",
    "    Connects to a ClearML Task and retrieves a pickled model artifact (e.g. from a Pipeline step).\n",
    "    \n",
    "    Args:\n",
    "        task_id (str): The ID of the specific task (e.g. the \"Model Training\" step).\n",
    "        artifact_name (str): The name of the artifact to retrieve. Defaults to \"trained Model\" \n",
    "                             (matches 'return_values' in the pipeline component).\n",
    "\n",
    "    Returns:\n",
    "        The deserialized model object (e.g. LitResNet).\n",
    "    \"\"\"\n",
    "    print(f\"Connecting to task: {task_id}\")\n",
    "    task = Task.get_task(task_id=task_id)\n",
    "    \n",
    "    if artifact_name in task.artifacts:\n",
    "        print(f\"Found artifact: '{artifact_name}'. Downloading and deserializing...\")\n",
    "        # .get() downloads the pickle and returns the Python object\n",
    "        return task.artifacts[artifact_name].get()\n",
    "    else:\n",
    "        available_artifacts = list(task.artifacts.keys())\n",
    "        raise ValueError(\n",
    "            f\"Artifact '{artifact_name}' not found in task {task_id}.\\n\"\n",
    "            f\"Available artifacts: {available_artifacts}\"\n",
    "        )\n",
    "    \n",
    "baseline    = load_model_from_task(\"3cebe0e4059e4fa58c57bd9a650ef7f5\", \"Baseline Model\")\n",
    "unlearn_ds  = load_model_from_task(\"ce89c3a50dda4ef1809314d2bce71374\", \"Test Dataloader\")\n",
    "test_dl     = load_model_from_task(\"ce89c3a50dda4ef1809314d2bce71374\", \"Unlearning Dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8326a555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to Pipeline: dd5a479f9048425481e993af74648693...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Task' object has no attribute 'get_pipeline_details'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 65\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m target_model, unlearn_ds, test_loader\n\u001b[0;32m     64\u001b[0m ID \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdd5a479f9048425481e993af74648693\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 65\u001b[0m _, _ , _ \u001b[38;5;241m=\u001b[39m \u001b[43mload_artifacts_from_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mID\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 31\u001b[0m, in \u001b[0;36mload_artifacts_from_pipeline\u001b[1;34m(pipeline_id)\u001b[0m\n\u001b[0;32m     26\u001b[0m train_task \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Iterate through child tasks to find the correct steps\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Note: ClearML pipelines usually name steps as \"StepName.TaskName\" or similar, \u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# but strictly we search by the task names defined in your PipelineDecorator.\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[43mpipeline_task\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_pipeline_details\u001b[49m()\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msteps\u001b[39m\u001b[38;5;124m'\u001b[39m, []):\n\u001b[0;32m     32\u001b[0m     step_name \u001b[38;5;241m=\u001b[39m step[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     33\u001b[0m     task_id \u001b[38;5;241m=\u001b[39m step[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtask_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Task' object has no attribute 'get_pipeline_details'"
     ]
    }
   ],
   "source": [
    "from clearml import Task, PipelineController\n",
    "import torch\n",
    "\n",
    "def load_artifacts_from_pipeline(pipeline_id: str):\n",
    "    \"\"\"\n",
    "    Fetches the necessary artifacts (Model, Unlearning Dataset, Test Loader) \n",
    "    from a specific ClearML Pipeline execution for finetuning the unlearning step.\n",
    "    \n",
    "    Args:\n",
    "        pipeline_id (str): The ID of the pipeline controller task (from the Web UI).\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (target_model, unlearn_ds, test_loader)\n",
    "    \"\"\"\n",
    "    print(f\"Connecting to Pipeline: {pipeline_id}...\")\n",
    "    pipeline_task = Task.get_task(task_id=pipeline_id)\n",
    "    \n",
    "    # Get the list of steps (tasks) created by this pipeline\n",
    "    # The pipeline controller tracks which tasks it created.\n",
    "    # We look for tasks named \"Train Baseline\" and \"Preprocess Data\" \n",
    "    # that belong to this pipeline instance.\n",
    "    \n",
    "    # 1. Find the Preprocessing Task (outputs datasets)\n",
    "    preprocess_task = None\n",
    "    # 2. Find the Training Task (outputs the model)\n",
    "    train_task = None\n",
    "\n",
    "    # Iterate through child tasks to find the correct steps\n",
    "    # Note: ClearML pipelines usually name steps as \"StepName.TaskName\" or similar, \n",
    "    # but strictly we search by the task names defined in your PipelineDecorator.\n",
    "    for step in pipeline_task.get_pipeline_details().get('steps', []):\n",
    "        step_name = step['name']\n",
    "        task_id = step['task_id']\n",
    "        \n",
    "        if step_name == \"Preprocess Data\":\n",
    "            preprocess_task = Task.get_task(task_id=task_id)\n",
    "        elif step_name == \"Train Baseline\":\n",
    "            train_task = Task.get_task(task_id=task_id)\n",
    "\n",
    "    if not preprocess_task or not train_task:\n",
    "        raise ValueError(\"Could not find 'Preprocess Data' or 'Train Baseline' steps in this pipeline.\")\n",
    "\n",
    "    print(f\"Found Preprocess Task: {preprocess_task.id}\")\n",
    "    print(f\"Found Training Task: {train_task.id}\")\n",
    "\n",
    "    # --- Load Artifacts ---\n",
    "    \n",
    "    # 1. Load Datasets from \"Preprocess Data\"\n",
    "    # The artifact names must match the 'return_values' in your @PipelineDecorator.component\n",
    "    print(\"Loading 'Unlearning Dataset'...\")\n",
    "    unlearn_ds = preprocess_task.artifacts['Unlearning Dataset'].get()\n",
    "    \n",
    "    print(\"Loading 'Test Dataloader'...\")\n",
    "    test_loader = preprocess_task.artifacts['Test Dataloader'].get()\n",
    "\n",
    "    # 2. Load Model from \"Train Baseline\"\n",
    "    # artifact name matches 'return_values=[\"Baseline Model\", ...]'\n",
    "    print(\"Loading 'Baseline Model'...\")\n",
    "    target_model = train_task.artifacts['Baseline Model'].get()\n",
    "\n",
    "    print(\"âœ… All artifacts loaded successfully.\")\n",
    "    return target_model, unlearn_ds, test_loader\n",
    "\n",
    "ID = \"dd5a479f9048425481e993af74648693\"\n",
    "_, _ , _ = load_artifacts_from_pipeline(ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ccc247",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 1, 3])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "tensor = torch.tensor([[1.0, 2.0, 3.0]])\n",
    "tensor.shape\n",
    "\n",
    "f = [8]\n",
    "f.extend(tensor.squeeze(0).shape)\n",
    "v = torch.randn(f) # Original vector of size 10\n",
    "v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c86f230a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset index from data/mnist_index.csv...\n",
      "Loaded Unlearning Dataset (test split using 'f1_split'): 889 Forget samples and 9111 Non-Forget samples.\n"
     ]
    }
   ],
   "source": [
    "from src.data.dataset_loaders import TrainTestDataset, UnlearningDataLoader, UnlearningPairDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Create the dataset\n",
    "unlearning_train_set = UnlearningPairDataset(\n",
    "    csv_file=\"data/mnist_index.csv\", \n",
    "    root_dir=\"data/softtarget_dataset/mnist\",\n",
    "    split='test'\n",
    ")\n",
    "\n",
    "# Use the custom DataLoader\n",
    "unlearning_loader = UnlearningDataLoader(\n",
    "    unlearning_train_set, \n",
    "    batch_size=2, \n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2527687",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3151"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unlearning_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a95d93b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]): '0',\n",
       " tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]): '1',\n",
       " tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]): '2',\n",
       " tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]): '3',\n",
       " tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]): '4',\n",
       " tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]): '5',\n",
       " tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]): '6',\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]): '7',\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]): '8',\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]): '9'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlearning_loader.dataset.tensor_to_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0ffddcc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset index from data/mnist_index.csv...\n",
      "Loaded test split (mode='forget', classes=['7', '8', '9']) with 6302 samples.\n"
     ]
    }
   ],
   "source": [
    "ttd = TrainTestDataset(\n",
    "    csv_file=\"data/mnist_index.csv\", \n",
    "    root_dir=\"data/softtarget_dataset/mnist\",\n",
    "    split='test',\n",
    "    sample_mode='forget',\n",
    "    classes=['7', '8', '9']\n",
    ")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "softtargets",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
