# configs/unlearning/graddiff.yaml

unlearning:
  epochs: 10
  batch_size: 8         # Should be the same as training batch size for consistency
  learning_rate: 0.00001
  optimizer: SGD        # Should be ADAM in the next version
  momentum: 0.9
  alpha: 0.5            # Weighting factor between retain (1-alpha) and forget (-alpha) loss